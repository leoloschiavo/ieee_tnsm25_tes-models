{"cells":[{"cell_type":"markdown","metadata":{"id":"l8Q8pYyRqFQo"},"source":["# ***Installation Requirements***\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Y1czngSveec"},"outputs":[],"source":["!pip install tensorflow==2.12.0"]},{"cell_type":"markdown","metadata":{"id":"h9E0aE5QrdZY"},"source":["# ***Mount Google Drive***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qA4zUfv_vkBp"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My\\ Drive/ieee_tnsm25_tes-models/TES-RNN/Antenna_Forecasting"]},{"cell_type":"markdown","metadata":{"id":"2ypeVqGAuSRZ"},"source":["# ***Imports***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dkRuMNhqwFw"},"outputs":[],"source":["import os\n","import math\n","import time\n","import torch\n","import pickle\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from data_loading import create_dataset, Dataset\n","from config import get_config\n","from trainer import TESRNNTrainer\n","from validator import TESRNNValidator\n","from tester import TESRNNTester\n","from model import TESRNN\n","from loss_modules import *"]},{"cell_type":"markdown","metadata":{"id":"mBI-C5mczUjW"},"source":["# ***TES-RNN***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TKcS_0NreF_"},"outputs":[],"source":["# CONFIGURATION SETTINGS\n","\n","# List of the services to be tested\n","services = ['Facebook', 'Instagram', 'Snapchat']\n","\n","# Base Station to be tested\n","bs_file = '/(35,3).npy'\n","bs_folder = '/BS_(35,3)'\n","\n","# Number of clusters (single cell prediction)\n","num_clusters = 1\n","\n","# List of dropout probabilities to be tested\n","ps_dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","# Validation runs for uncertainty\n","uncertainty_runs = 10\n","\n","# Test runs for uncertainty\n","uncertainty_test_runs = 100\n","\n","# Define the number of training epochs\n","epochs = 20\n","\n","# Define the number of training batch size\n","batch_size = 288\n","\n","# Define the number of train, validation and test samples\n","train_samples = 16128\n","val_samples = 4032\n","test_samples = 2016\n","\n","# Define the input size and output size of the prediction\n","input_size = 6\n","output_size = 1\n","\n","# Define simulation run seed\n","num_run = 0\n","torch.manual_seed(num_run)\n","\n","# Define the device type\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1n3s1q6GEYZG"},"outputs":[],"source":["# ACTOR-CRITIC IMPORTS AND SETTINGS\n","from sac import SAC\n","from Utils.buffer import ReplayBuffer\n","from torch.autograd import Variable\n","\n","# Define state, action size and agents\n","state_size = 2\n","action_size = 21\n","possible_tau = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n","num_agents = 1\n","\n","# Define general parameters\n","gamma = 0.99\n","tau_ac = 0.001\n","rollout_threads = 1\n","ac_batch_size = 10\n","pol_hidden_dim = 512\n","critic_hidden_dim = 512\n","pi_lr = 0.01\n","q_lr = 0.01\n","norm_rews = True\n","\n","# Define Actor-Critic iterations and experience-buffer length\n","ac_iterations = 50\n","buffer_length = ac_iterations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkKJiRG10k0w"},"outputs":[],"source":["# SIMULATION RUNS\n","\n","# Iterate over the services\n","for service in services:\n","\n","    # Simulations over different dropout probabilities\n","    for p_dropout in ps_dropout:\n","\n","                # Obtain the data of the simulation\n","                config = get_config('Traffic', epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, input_size, output_size)\n","\n","                # Data loading\n","                data = '../../../Dataset/' + service + bs_file\n","                train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'])\n","                dataset = Dataset(train, val, test, config['device'])\n","\n","                # Set the maximum as the maximum in the training set\n","                maximum = np.max(train[0])\n","\n","                # Get the average of the validation traffic val\n","                avg_val = np.mean(val[0])\n","                std_val = np.std(val[0])\n","\n","                # Single model per configuration case\n","                max_avg_val = avg_val\n","                max_std_val = std_val\n","\n","                # Turn on exploration\n","                exploration = True\n","\n","                # Actor Critic initialization\n","                ac_model = SAC.init_from_env(state_size, action_size, num_agents, gamma, tau_ac, pi_lr=pi_lr, q_lr=q_lr, pol_hidden_dim=pol_hidden_dim, critic_hidden_dim=critic_hidden_dim)\n","                replay_buffer = ReplayBuffer(buffer_length, num_agents, [(state_size) for j in range(num_agents)], [action_size for k in range(num_agents)])\n","\n","                # Prepare the model\n","                ac_model.prep_rollouts(device='cpu')\n","\n","                for ac_iter in range(ac_iterations):\n","\n","                    # Check whether turn off exploration\n","                    if ac_iter >= (ac_iterations - 5) :\n","                        exploration = False\n","\n","                    # Get the average value in the validation dataset (normalized state of the Actor Critic)\n","                    avg_validation_traffic = avg_val / max_avg_val\n","\n","                    # Get the standard deviation value in the validation dataset (normalized state of the Actor Critic)\n","                    std_validation_traffic = std_val / max_std_val\n","\n","\n","                    # Determine the state, action and next state for the Actor Critic\n","                    complete_agent_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                    agent_state = np.ndarray(shape=(state_size,))\n","                    agent_state[0] = avg_validation_traffic\n","                    agent_state[1] = std_validation_traffic\n","                    for k in range(num_agents):\n","                        complete_agent_state[0,k] = agent_state\n","                    torch_state = [Variable(torch.Tensor(np.vstack(complete_agent_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","\n","\n","                    # Get the action from the Actor Critic\n","                    torch_action = ac_model.step(torch_state, explore=exploration)\n","                    agent_actions = [ac.data.numpy() for ac in torch_action]\n","                    tau = possible_tau[np.argmax(agent_actions[0])]\n","\n","                    # Dataloader initialization\n","                    dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n","\n","                    # Model initialization\n","                    run_id = service + bs_folder + '/Dropout_' + str(p_dropout) + '/Simulation_' + str(num_run)\n","                    model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id, p_dropout = p_dropout)\n","\n","                    # Run model trainer\n","                    trainer = TESRNNTrainer(model, dataloader, run_id, config)\n","                    trainer.train_epochs()\n","\n","                    # Run model validator with uncertainty\n","                    uncertain_val_losses = []\n","                    for val_run in range(uncertainty_runs):\n","                        validator = TESRNNValidator(model, dataloader, run_id, config)\n","                        validator.validating()\n","\n","                        # Compute denormalized validation loss\n","                        norm_preds = np.load('Results/' + run_id + '/val_predictions.npy')\n","                        norm_actuals = np.load('Results/' + run_id + '/val_actuals.npy')\n","                        levels = np.load('Results/' + run_id + '/val_levels.npy')\n","                        val_loss = denorm_validation_loss(norm_preds, norm_actuals, levels)\n","                        uncertain_val_losses.append(val_loss)\n","\n","                    # Compute the average validation loss\n","                    val_loss = np.average(np.array(uncertain_val_losses))\n","\n","\n","                    # Determine the next state for the Actor Critic\n","                    complete_agent_post_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                    agent_post_state = np.ndarray(shape=(state_size,))\n","                    agent_post_state[0] = avg_validation_traffic\n","                    agent_post_state[1] = std_validation_traffic\n","                    for k in range(num_agents):\n","                        complete_agent_post_state[0,k] = agent_post_state\n","                    torch_post_state = [Variable(torch.Tensor(np.vstack(complete_agent_post_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","                    rewards = np.ndarray(shape=(rollout_threads, num_agents))\n","                    rewards[0,0] = -val_loss\n","\n","\n","                    # Save the experience in the replay buffer\n","                    replay_buffer.push(complete_agent_state, agent_actions, rewards, complete_agent_post_state)\n","\n","\n","\t\t            # Perform training of the Radio agent model\n","                    if len(replay_buffer) >= (ac_batch_size) and ac_iter < (ac_iterations - 5):\n","                        ac_model.prep_training(device='cpu')\n","                        sample = replay_buffer.sample(ac_batch_size, to_gpu=False, norm_rews=norm_rews)\n","                        ac_model.update_critic(sample)\n","                        ac_model.update_policies(sample)\n","                        ac_model.update_all_targets()\n","                        ac_model.prep_rollouts(device='cpu')\n","\n","                        # Perform a sample inference\n","                        complete_agent_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                        agent_state = np.ndarray(shape=(state_size,))\n","                        agent_state[0] = avg_validation_traffic\n","                        agent_state[1] = std_validation_traffic\n","                        for k in range(num_agents):\n","                            complete_agent_state[0,k] = agent_state\n","                        torch_state = [Variable(torch.Tensor(np.vstack(complete_agent_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","                        torch_action = ac_model.step(torch_state, explore=False)\n","                        agent_actions = [ac.data.numpy() for ac in torch_action]\n","                        test_tau = possible_tau[np.argmax(agent_actions[0])]\n","                        np.save('Results/' + run_id + '/test_tau_ac_iter_' + str(ac_iter) + '.npy', tau)\n","\n","\n","                # Save the model\n","                file_path = os.path.join('AC_Models/', run_id)\n","                model_path = os.path.join(file_path, 'ac_model_sim_' + str(num_run))\n","                os.makedirs(file_path, exist_ok=True)\n","                ac_model.save('AC_Models/' + run_id + '/ac_model_sim_' + str(num_run))\n","\n","\n","\n","                # Run the optimized model after taus optimization\n","\n","                # Dataloader initialization\n","                dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n","\n","                # Model initialization\n","                run_id = service + bs_folder + '/Dropout_' + str(p_dropout) + '/Simulation_' + str(num_run)\n","                model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id, p_dropout = p_dropout)\n","\n","                # Run model trainer\n","                trainer = TESRNNTrainer(model, dataloader, run_id, config)\n","                trainer.train_epochs()\n","\n","                # Run model tester with uncertainty\n","                uncertain_test_losses = []\n","                for test_run in range(uncertainty_test_runs):\n","                    tester = TESRNNTester(model, dataloader, run_id, config, test_run)\n","                    predictions, actuals = tester.testing()\n","\n","                    # Move to numpy arrays\n","                    predictions = predictions.cpu()\n","                    actuals = actuals.cpu()\n","\n","                    # Denormalize the predictions and actuals\n","                    levels = np.load('Results/' + run_id + '/test_levels_' + str(test_run) + '.npy')\n","                    predictions = predictions[:,0,0] * levels\n","                    actuals = actuals[:,0,0] * levels\n","\n","                    # Move to numpy arrays\n","                    predictions = predictions.cpu().numpy()\n","                    actuals = actuals.cpu().numpy()\n","\n","                    # Find the different parts of alphaloss\n","                    den_loss = np.square(np.subtract(actuals,predictions)).mean()\n","                    print(\"Denormalized MSE-loss: \", den_loss)\n","\n","                    # Store the results\n","                    np.save('Results/' + run_id + '/tes-rnn_predictions_%s_%d_%d_dropout_%s.npy'%(service, num_run, test_run, p_dropout), predictions)\n","                    np.save('Results/' + run_id + '/tes-rnn_actuals_%s_%d_%d_dropout_%s.npy'%(service, num_run, test_run, p_dropout), actuals)\n","                    np.save('Results/' + run_id + '/tes-rnn_mse_loss_%s_%d_%d_dropout_%s.npy'%(service, num_run, test_run, p_dropout), den_loss)\n","\n","                np.save('Results/' + run_id + '/tes-rnn_tau_%s_%d_dropout_%s.npy'%(service, num_run, p_dropout), tau)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"ES-NN","language":"python","name":"es-nn"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}