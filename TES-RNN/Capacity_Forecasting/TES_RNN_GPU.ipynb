{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["l8Q8pYyRqFQo","h9E0aE5QrdZY"]},"kernelspec":{"display_name":"ES-NN","language":"python","name":"es-nn"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","source":["# ***Installation Requirements***\n","\n"],"metadata":{"id":"l8Q8pYyRqFQo"}},{"cell_type":"code","metadata":{"id":"3Y1czngSveec","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1751477488336,"user_tz":-120,"elapsed":75468,"user":{"displayName":"LEONARDO LO SCHIAVO","userId":"12816926737253820683"}},"outputId":"50841def-7742-4518-b4d5-20fe5e572a0e"},"source":["!pip install tensorflow==2.12.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.12.0\n","  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.73.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.14.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n","Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n","  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n","Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n","  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n","Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n","  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n","Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n","  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.14.0)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n","  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n","Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n","INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n","Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n","  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n","  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n","Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n","  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n","  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.6.15)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n","Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.2\n","    Uninstalling wrapt-1.17.2:\n","      Successfully uninstalled wrapt-1.17.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.8.0\n","    Uninstalling keras-3.8.0:\n","      Successfully uninstalled keras-3.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.6.0\n","    Uninstalling gast-0.6.0:\n","      Successfully uninstalled gast-0.6.0\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.5.1\n","    Uninstalling jaxlib-0.5.1:\n","      Successfully uninstalled jaxlib-0.5.1\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.2\n","    Uninstalling google-auth-oauthlib-1.2.2:\n","      Successfully uninstalled google-auth-oauthlib-1.2.2\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.18.0\n","    Uninstalling tensorboard-2.18.0:\n","      Successfully uninstalled tensorboard-2.18.0\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.5.2\n","    Uninstalling jax-0.5.2:\n","      Successfully uninstalled jax-0.5.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.18.0\n","    Uninstalling tensorflow-2.18.0:\n","      Successfully uninstalled tensorflow-2.18.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","blosc2 3.5.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n","scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","orbax-checkpoint 0.11.16 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n","albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","bigframes 2.8.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n","flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n","db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.8 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]},"id":"3b9eaaaedcd840a99e958dd9e234d2ce"}},"metadata":{}}]},{"cell_type":"markdown","source":["# ***Mount Google Drive***"],"metadata":{"id":"h9E0aE5QrdZY"}},{"cell_type":"code","metadata":{"id":"qA4zUfv_vkBp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751477524385,"user_tz":-120,"elapsed":22247,"user":{"displayName":"LEONARDO LO SCHIAVO","userId":"12816926737253820683"}},"outputId":"060894f8-3ff5-4ac7-968c-6584caffd5d4"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My\\ Drive/ieee_tnsm25_tes-models/TES-RNN/Capacity_Forecasting"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/My Drive/ieee_tnsm25_tes-models/TES-RNN/Capacity_Forecasting\n"]}]},{"cell_type":"markdown","source":["# ***Imports***"],"metadata":{"id":"2ypeVqGAuSRZ"}},{"cell_type":"code","metadata":{"id":"4dkRuMNhqwFw"},"source":["import os\n","import math\n","import time\n","import torch\n","import pickle\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from data_loading import create_dataset, Dataset\n","from config import get_config\n","from trainer import TESRNNTrainer\n","from validator import TESRNNValidator\n","from tester import TESRNNTester\n","from model import TESRNN\n","from loss_modules import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***TES-RNN***"],"metadata":{"id":"mBI-C5mczUjW"}},{"cell_type":"code","metadata":{"id":"4TKcS_0NreF_"},"source":["# CONFIGURATION SETTINGS\n","\n","# List of the services to be tested\n","services = ['Facebook', 'Instagram', 'Snapchat']\n","\n","# Number of clusters types\n","num_clusterss = [1]\n","\n","# List of alphas to be tested\n","alphas = [1, 2, 3, 5]\n","\n","# Define the number of training epochs\n","epochs = 20\n","\n","# Define the number of training batch size\n","batch_size = 288\n","\n","# Define the number of train, validation and test samples\n","train_samples = 16128\n","val_samples = 4032\n","test_samples = 2016\n","\n","# Define the input size and output size of the prediction\n","input_size = 6\n","output_size = 1\n","\n","# Define simulation run seed\n","num_run = 0\n","torch.manual_seed(num_run)\n","\n","# Define the device type\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ACTOR-CRITIC IMPORTS AND SETTINGS\n","from sac import SAC\n","from Utils.buffer import ReplayBuffer\n","from torch.autograd import Variable\n","\n","# Define state, action size and agents\n","state_size = 2\n","action_size = 21\n","possible_tau = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n","num_agents = 1\n","\n","# Define general parameters\n","gamma = 0.99\n","tau_ac = 0.001\n","rollout_threads = 1\n","ac_batch_size = 10\n","pol_hidden_dim = 512\n","critic_hidden_dim = 512\n","pi_lr = 0.01\n","q_lr = 0.01\n","norm_rews = True\n","\n","# Define Actor-Critic iterations and experience-buffer length\n","ac_iterations = 50\n","buffer_length = ac_iterations"],"metadata":{"id":"1n3s1q6GEYZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SIMULATION RUNS\n","\n","# Iterate over the services\n","for service in services:\n","\n","    # Iterate over the number of clusters types\n","    for num_clusters in num_clusterss:\n","\n","        # Iterate over the alphas\n","        for alpha in alphas:\n","\n","            # Iterate for the number of clusters\n","            for run in range(num_clusters):\n","\n","                # Obtain the data of the simulation\n","                config = get_config('Traffic', epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, alpha, input_size, output_size)\n","                if num_clusters > 1:\n","                    data = '../../../Dataset/' + service + '/time_load_cor_matrix_' + str(num_clusters) + '_clust.npy'\n","                    train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'], clustering=True, cluster=run)\n","                else:\n","                    data = '../../../Dataset/' + service + '/time_load_cor_matrix.npy'\n","                    train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'])\n","\n","\n","                # Get the maximums and the datasets\n","                max_train = np.max(train[0])\n","                max_val = np.max(val[0])\n","                max_test = np.max(test[0])\n","                dataset = Dataset(train, val, test, config['device'])\n","\n","                # Set the maximum as the maximum in the training set\n","                maximum = max_train\n","\n","\n","                # Get the average of the validation traffic val\n","                avg_val = np.mean(val[0])\n","                std_val = np.std(val[0])\n","\n","                # Single model per configuration case\n","                max_avg_val = avg_val\n","                max_std_val = std_val\n","\n","                # Turn on exploration\n","                exploration = True\n","\n","                # Actor Critic initialization\n","                ac_model = SAC.init_from_env(state_size, action_size, num_agents, gamma, tau_ac, pi_lr=pi_lr, q_lr=q_lr, pol_hidden_dim=pol_hidden_dim, critic_hidden_dim=critic_hidden_dim)\n","                replay_buffer = ReplayBuffer(buffer_length, num_agents, [(state_size) for j in range(num_agents)], [action_size for k in range(num_agents)])\n","\n","                # Prepare the model\n","                ac_model.prep_rollouts(device='cpu')\n","\n","                for ac_iter in range(ac_iterations):\n","\n","                    # Check whether turn off exploration\n","                    if ac_iter >= (ac_iterations - 5) :\n","                        exploration = False\n","\n","                    # Get the average value in the validation dataset (normalized state of the Actor Critic)\n","                    avg_validation_traffic = avg_val / max_avg_val\n","\n","                    # Get the standard deviation value in the validation dataset (normalized state of the Actor Critic)\n","                    std_validation_traffic = std_val / max_std_val\n","\n","\n","                    # Determine the state, action and next state for the Actor Critic\n","                    complete_agent_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                    agent_state = np.ndarray(shape=(state_size,))\n","                    agent_state[0] = avg_validation_traffic\n","                    agent_state[1] = std_validation_traffic\n","                    for k in range(num_agents):\n","                        complete_agent_state[0,k] = agent_state\n","                    torch_state = [Variable(torch.Tensor(np.vstack(complete_agent_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","\n","\n","                    # Get the action from the Actor Critic\n","                    torch_action = ac_model.step(torch_state, explore=exploration)\n","                    agent_actions = [ac.data.numpy() for ac in torch_action]\n","                    tau = possible_tau[np.argmax(agent_actions[0])]\n","\n","                    # Dataloader initialization\n","                    dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n","\n","                    # Model initialization\n","                    run_id = service + '/Alpha_' + str(alpha) + '/Simulation_' + str(num_run)\n","                    model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id)\n","\n","                    # Run model trainer\n","                    trainer = TESRNNTrainer(model, dataloader, run_id, config)\n","                    trainer.train_epochs()\n","\n","                    # Run model validator\n","                    validator = TESRNNValidator(model, dataloader, run_id, config)\n","                    validator.validating()\n","\n","                    # Compute denormalized validation loss\n","                    norm_preds = np.load('Results/' + run_id + '/val_predictions.npy')\n","                    norm_actuals = np.load('Results/' + run_id + '/val_actuals.npy')\n","                    levels = np.load('Results/' + run_id + '/val_levels.npy')\n","                    val_loss = denorm_validation_loss(norm_preds, norm_actuals, levels, alpha)\n","\n","\n","\n","                    # Determine the next state for the Actor Critic\n","                    complete_agent_post_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                    agent_post_state = np.ndarray(shape=(state_size,))\n","                    agent_post_state[0] = avg_validation_traffic\n","                    agent_post_state[1] = std_validation_traffic\n","                    for k in range(num_agents):\n","                        complete_agent_post_state[0,k] = agent_post_state\n","                    torch_post_state = [Variable(torch.Tensor(np.vstack(complete_agent_post_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","                    rewards = np.ndarray(shape=(rollout_threads, num_agents))\n","                    rewards[0,0] = -val_loss\n","\n","\n","                    # Save the experience in the replay buffer\n","                    replay_buffer.push(complete_agent_state, agent_actions, rewards, complete_agent_post_state)\n","\n","\n","\t\t            # Perform training of the Radio agent model\n","                    if len(replay_buffer) >= (ac_batch_size) and ac_iter < (ac_iterations - 5):\n","                        ac_model.prep_training(device='cpu')\n","                        sample = replay_buffer.sample(ac_batch_size, to_gpu=False, norm_rews=norm_rews)\n","                        ac_model.update_critic(sample)\n","                        ac_model.update_policies(sample)\n","                        ac_model.update_all_targets()\n","                        ac_model.prep_rollouts(device='cpu')\n","\n","                        # Perform a sample inference\n","                        complete_agent_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                        agent_state = np.ndarray(shape=(state_size,))\n","                        agent_state[0] = avg_validation_traffic\n","                        agent_state[1] = std_validation_traffic\n","                        for k in range(num_agents):\n","                            complete_agent_state[0,k] = agent_state\n","                        torch_state = [Variable(torch.Tensor(np.vstack(complete_agent_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","                        torch_action = ac_model.step(torch_state, explore=False)\n","                        agent_actions = [ac.data.numpy() for ac in torch_action]\n","                        test_tau = possible_tau[np.argmax(agent_actions[0])]\n","                        np.save('Results/' + run_id + '/test_tau_ac_iter_' + str(ac_iter) + '.npy', tau)\n","\n","\n","                # Save the model\n","                file_path = os.path.join('AC_Models/', run_id)\n","                model_path = os.path.join(file_path, 'ac_model_sim_' + str(num_run))\n","                os.makedirs(file_path, exist_ok=True)\n","                ac_model.save('AC_Models/' + run_id + '/ac_model_sim_' + str(num_run))\n","\n","\n","\n","                # Run the optimized model after taus optimization\n","\n","                # Dataloader initialization\n","                dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n","\n","                # Model initialization\n","                run_id = service + '/Alpha_' + str(alpha) + '/Simulation_' + str(num_run)\n","                model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id)\n","\n","                # Run model trainer\n","                trainer = TESRNNTrainer(model, dataloader, run_id, config)\n","                trainer.train_epochs()\n","\n","                # Run model tester\n","                tester = TESRNNTester(model, dataloader, run_id, config, service, num_clusters, run)\n","                predictions, actuals = tester.testing()\n","\n","                # Move to numpy arrays\n","                predictions = predictions.cpu()\n","                actuals = actuals.cpu()\n","\n","                # Denormalize the predictions and actuals\n","                levels = np.load('Results/' + run_id + '/test_levels.npy')\n","                predictions = predictions[:,0,0] * levels\n","                actuals = actuals[:,0,0] * levels\n","\n","                # Find the peak\n","                peak = torch.max(actuals)\n","\n","                # Move to numpy arrays\n","                predictions = predictions.cpu().numpy()\n","                actuals = actuals.cpu().numpy()\n","                peak = peak.cpu().numpy()\n","\n","                # Find the different parts of alphaloss\n","                den_loss, over, sla = evaluate_costs_single_clust(predictions, actuals, peak, alpha)\n","                sla_cost = den_loss - over\n","                print(\"Denormalized Alpha-loss: \", den_loss)\n","\n","\n","                # Store the results\n","                np.save('Results/' + run_id + '/tes-rnn_predictions_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), predictions)\n","                np.save('Results/' + run_id + '/tes-rnn_actuals_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), actuals)\n","                np.save('Results/' + run_id + '/tes-rnn_alpha_loss_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), den_loss)\n","                np.save('Results/' + run_id + '/tes-rnn_over_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), over)\n","                np.save('Results/' + run_id + '/tes-rnn_sla_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), sla_cost)\n","                np.save('Results/' + run_id + '/tes-rnn_tau_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), tau)\n"],"metadata":{"id":"FkKJiRG10k0w","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"error","timestamp":1751479754559,"user_tz":-120,"elapsed":769149,"user":{"displayName":"LEONARDO LO SCHIAVO","userId":"12816926737253820683"}},"outputId":"0dba4c2b-1211-49c3-fb30-5c00c34507ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Denormalized Alpha-loss:  6273716225023.168\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-8-411085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# Run model trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTESRNNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;31m# Run model validator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/My Drive/ieee_tnsm25_tes-models/TES-RNN/Capacity_Forecasting/trainer.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/My Drive/ieee_tnsm25_tes-models/TES-RNN/Capacity_Forecasting/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Train over the clusters of the dataset (single cluster, single iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclust_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_clust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclust_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/My Drive/ieee_tnsm25_tes-models/TES-RNN/Capacity_Forecasting/trainer.py\u001b[0m in \u001b[0;36mtrain_clust\u001b[0;34m(self, train, val, test, idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtemp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemporal_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemporal_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_temporal_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mseries_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mseries_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/My Drive/ieee_tnsm25_tes-models/TES-RNN/Capacity_Forecasting/trainer.py\u001b[0m in \u001b[0;36mtrain_temporal_batch\u001b[0;34m(self, temp_train, val, test, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Compute loss and perform backward step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_clipping'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}