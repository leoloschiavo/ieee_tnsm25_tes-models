{"cells":[{"cell_type":"markdown","metadata":{"id":"l8Q8pYyRqFQo"},"source":["# ***Installation Requirements***\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Y1czngSveec"},"outputs":[],"source":["!pip install tensorflow==2.12.0"]},{"cell_type":"markdown","metadata":{"id":"h9E0aE5QrdZY"},"source":["# ***Mount Google Drive***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qA4zUfv_vkBp"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My\\ Drive/ieee_tnsm25_tes-models/TES-Transformer/Capacity_Forecasting"]},{"cell_type":"markdown","metadata":{"id":"2ypeVqGAuSRZ"},"source":["# ***Imports***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dkRuMNhqwFw"},"outputs":[],"source":["import os\n","import math\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pickle\n","import random\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from data_loading import create_dataset, Dataset\n","from config import get_config\n","from loss_modules import *"]},{"cell_type":"markdown","metadata":{"id":"mBI-C5mczUjW"},"source":["# ***TES-Transformer***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TKcS_0NreF_"},"outputs":[],"source":["# CONFIGURATION SETTINGS\n","\n","# List of the services to be tested\n","services = ['Facebook', 'Instagram', 'Snapchat']\n","\n","# Number of clusters types\n","num_clusterss = [1]\n","\n","# List of alphas to be tested\n","alphas = [1, 2, 3, 5]\n","\n","# Define the number of training epochs\n","epochs = 20\n","\n","# Define the number of training batch size\n","batch_size = 288\n","\n","# Define the number of train, validation and test samples\n","train_samples = 16128\n","val_samples = 4032\n","test_samples = 2016\n","\n","# Define the input window and output window of the prediction\n","input_window = 6\n","output_window = 1\n","block_len = input_window + output_window\n","\n","# Define simulation run seed\n","num_run = 0\n","torch.manual_seed(0)\n","\n","# Define the device type\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1n3s1q6GEYZG"},"outputs":[],"source":["# ACTOR-CRITIC IMPORTS AND SETTINGS\n","from sac import SAC\n","from Utils.buffer import ReplayBuffer\n","from torch.autograd import Variable\n","\n","# Define state, action size and agents\n","state_size = 2\n","action_size = 20\n","possible_tau = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n","num_agents = 1\n","\n","# Define general parameters\n","gamma = 0.99\n","tau_ac = 0.001\n","rollout_threads = 1\n","ac_batch_size = 10\n","pol_hidden_dim = 512\n","critic_hidden_dim = 512\n","pi_lr = 0.01\n","q_lr = 0.01\n","norm_rews = True\n","\n","# Define Actor-Critic iterations and experience-buffer length\n","ac_iterations = 50\n","buffer_length = ac_iterations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VP3M9G2GUr4"},"outputs":[],"source":["# TES-Transformer implementation\n","\n","# Implementation of the Positional Encoding module\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = 1 / (10000 ** ((2 * np.arange(d_model)) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term[0::2])\n","        pe[:, 1::2] = torch.cos(position * div_term[1::2])\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return x + self.pe[:x.size(0), :]\n","\n","\n","# Implementation of the Transformer Attention module\n","class TransAm(nn.Module):\n","    def __init__(self, maximum, feature_size=10, num_layers=1, dropout=0.1):\n","        super(TransAm, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.input_embedding = nn.Linear(6, feature_size)\n","        self.pos_encoder = PositionalEncoding(feature_size)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        self.decoder = nn.Linear(feature_size, 1)\n","        self.init_weights()\n","        self.maximum = maximum\n","        self.logistic = nn.Sigmoid()\n","\n","        #Set initial parameter of the Exponential Smoothing formula and make it learnable\n","        init_lev_sms = []\n","        init_lev_sms.append(nn.Parameter(torch.Tensor([0.5]), requires_grad=True))\n","        self.init_lev_sms = nn.ParameterList(init_lev_sms)\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src):\n","        if src.dim() == 3 and src.size(-1) == 1:\n","            src = src.squeeze(-1)\n","\n","        src = self.input_embedding(src)\n","        src = src.unsqueeze(0)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src)\n","        output = self.decoder(output)\n","        return output.squeeze(0)\n","\n","\n","# Implementation of the module for Input/Output sequences\n","def create_inout_sequences(input_data, input_window ,output_window):\n","    inout_seq = []\n","    L = len(input_data)\n","    block_num =  L - block_len + 1\n","\n","    for i in range( block_num ):\n","        train_seq = input_data[i : i + input_window]\n","        train_label = input_data[i + output_window : i + input_window + output_window]\n","        inout_seq.append((train_seq ,train_label))\n","\n","    return torch.FloatTensor(np.array(inout_seq))\n","\n","\n","# Implementation of the module for data retrieval\n","def get_data(epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, alpha, input_window, output_window, service, run):\n","\n","    config = get_config('Traffic', epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, alpha, input_window, output_window)\n","    if num_clusters > 1:\n","        data = '../../../Dataset/' + service + '/time_load_cor_matrix_' + str(num_clusters) + '_clust.npy'\n","        train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'], clustering=True, cluster=run)\n","    else:\n","        data = '../../../Dataset/' + service + '/time_load_cor_matrix.npy'\n","        train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'])\n","\n","    train_data = train[0]\n","    val_data = val[0]\n","    test_data = test[0]\n","    max_train = np.max(train[0])\n","    max_val = np.max(val[0])\n","    mean_val = np.mean(val[0])\n","    std_val = np.std(val[0])\n","    max_test = np.max(test[0])\n","\n","    train_sequence = create_inout_sequences(train_data, input_window, output_window)\n","    val_sequence = create_inout_sequences(val_data, input_window, output_window)\n","    test_sequence = create_inout_sequences(test_data, input_window, output_window)\n","\n","    return train_sequence.to(device), val_sequence.to(device), test_sequence.to(device), max_train, max_val, mean_val, std_val, max_test\n","\n","\n","# Implementation of the module for batch data retrieval\n","def get_batch(model, input_data, i , batch_size, tau):\n","\n","    input_data = input_data + 0.1\n","    batch_len = min(batch_size, len(input_data) - i)\n","    data = input_data[ i:i + batch_len ]\n","    lev_sms = model.logistic(torch.stack([model.init_lev_sms[idx] for idx in [0]]).squeeze(1))\n","\n","    # Calculate initial levels\n","    levels = []\n","    max_levels = []\n","    init_lev = input_data[0, 0, 0]\n","    levels.append(init_lev)\n","\n","    # Calculate level for the current timestep\n","    for i in range(1, data.shape[0]):\n","        new_lev = lev_sms * (data[i,0,0]) + (1 - lev_sms) * levels[i - 1]\n","        levels.append(new_lev)\n","\n","    for i in range(data.shape[0], data.shape[0] + input_window):\n","        new_lev = lev_sms * (data[data.shape[0] - 1, 0, i - data.shape[0]]) + (1 - lev_sms) * levels[i - 1]\n","        levels.append(new_lev)\n","\n","    levels = torch.tensor(levels)\n","\n","    for i in range(data.shape[0]):\n","\n","        data[i,0,:] = torch.div(data[i,0,:],levels[i + input_window])\n","        data[i,1,:] = torch.div(data[i,1,:],levels[i + input_window])\n","        max_window = torch.max(data[i,0,:])*tau\n","        if max_window>1e-3:\n","            data[i,0,:] = torch.div(data[i,0,:], max_window)\n","            data[i,1,:] = torch.div(data[i,1,:], max_window)\n","            max_levels.append(max_window)\n","        else:\n","            data[i,0,:] = torch.div(data[i,0,:], 1)\n","            data[i,1,:] = torch.div(data[i,1,:], 1)\n","            max_levels.append(1)\n","    max_levels = torch.tensor(max_levels)\n","    input = torch.stack([item[0] for item in data]).view((batch_len, input_window))\n","    target = torch.stack([item[1][-1]for item in data]).view((batch_len, 1))\n","\n","    return input, target, levels, max_levels\n","\n","\n","# Implementation of the module for training\n","def train(model, optimizer, criterion, train_data, tau):\n","    model.train()\n","    total_loss = 0.\n","\n","    for batch, i in enumerate(range(0, len(train_data), batch_size)):\n","        data, targets, levels, max_levels = get_batch(model, train_data, i , batch_size, tau)\n","        one_window = np.array(data.cpu())[0]\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        log_interval = int(len(train_data) / batch_size / 5)\n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","\n","\n","# Implementation of the module for validation and testing\n","def evaluate(eval_model, criterion, data_source, epoch, tau):\n","    eval_model.eval()\n","    total_loss = 0.\n","    eval_result = torch.Tensor(0)\n","    truth = torch.Tensor(0)\n","    with torch.no_grad():\n","        numpy_levels = []\n","        numpy_max_levels = []\n","        for i in range(len(data_source)):\n","            data, target, levels, max_levels = get_batch(eval_model, data_source, i , 1, tau)\n","            output = eval_model(data)\n","            total_loss += criterion(output, target).item()\n","            eval_result = torch.cat((eval_result, output[-1].view(-1).cpu()), 0)\n","            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n","            numpy_levels.append(levels[-1].cpu())\n","            numpy_max_levels.append(max_levels[-1].cpu())\n","\n","\n","    final_levels = np.array(numpy_levels)\n","    final_max_levels = np.array(numpy_max_levels)\n","\n","    eval_result = eval_result * final_max_levels * final_levels\n","    eval_result = eval_result - 0.1\n","    truth = truth * final_levels * final_max_levels\n","    truth = truth - 0.1\n","\n","    return total_loss / i, eval_result, truth, final_levels, final_max_levels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkKJiRG10k0w"},"outputs":[],"source":["# SIMULATION RUNS\n","\n","# Iterate over the services\n","for service in services:\n","\n","    # Iterate over the number of antennas\n","    for num_clusters in num_clusterss:\n","\n","        # Iterate over the alphas\n","        for alpha in alphas:\n","\n","            # Iterate for the number of clusters\n","            for run in range(num_clusters):\n","\n","                # Obtain the data of the simulation\n","                train_data, val_data, test_data, max_train, max_val, avg_val, std_val, max_test = get_data(epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, alpha, input_window, output_window, service, run)\n","\n","                # Set the maximum\n","                maximum = max_train\n","\n","                # Single model per configuration case\n","                max_avg_val = avg_val\n","                max_std_val = std_val\n","\n","                # Turn on exploration\n","                exploration = True\n","\n","                # Actor Critic initialization\n","                ac_model = SAC.init_from_env(state_size, action_size, num_agents, gamma, tau_ac, pi_lr=pi_lr, q_lr=q_lr, pol_hidden_dim=pol_hidden_dim, critic_hidden_dim=critic_hidden_dim)\n","                replay_buffer = ReplayBuffer(buffer_length, num_agents, [(state_size) for j in range(num_agents)], [action_size for k in range(num_agents)])\n","\n","                # Prepare the model\n","                ac_model.prep_rollouts(device='cpu')\n","\n","                for ac_iter in range(ac_iterations):\n","\n","                    # Check whether turn off exploration\n","                    if ac_iter >= (ac_iterations - 5) :\n","                        exploration = False\n","\n","                    # Get the average value in the validation dataset (normalized state of the Actor Critic)\n","                    avg_validation_traffic = avg_val / max_avg_val\n","\n","                    # Get the standard deviation value in the validation dataset (normalized state of the Actor Critic)\n","                    std_validation_traffic = std_val / max_std_val\n","\n","\n","                    # Determine the state, action and next state for the Actor Critic\n","                    complete_agent_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                    agent_state = np.ndarray(shape=(state_size,))\n","                    agent_state[0] = avg_validation_traffic\n","                    agent_state[1] = std_validation_traffic\n","                    for k in range(num_agents):\n","                        complete_agent_state[0,k] = agent_state\n","                    torch_state = [Variable(torch.Tensor(np.vstack(complete_agent_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","\n","\n","                    # Get the action from the Actor Critic\n","                    torch_action = ac_model.step(torch_state, explore=exploration)\n","                    agent_actions = [ac.data.numpy() for ac in torch_action]\n","                    tau = possible_tau[np.argmax(agent_actions[0])]\n","\n","                    # Model initialization\n","                    run_id = service + '/Alpha_' + str(alpha) + '/Simulation_' + str(num_run)\n","                    model = TransAm(max_train).to(device)\n","\n","                    # Select loss function and optimizer and scheduler and learning rate\n","                    criterion = AlphaLoss(alpha, 1, \"cuda\")\n","                    lr = 0.005\n","                    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","                    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n","\n","                    # Run model trainer\n","                    for epoch in range(epochs):\n","                        train(model, optimizer, criterion, train_data, tau)\n","                        scheduler.step()\n","\n","                    # Run model validator\n","                    loss, predictions, actuals, final_levels, final_max_levels = evaluate(model, criterion, val_data, epochs, tau)\n","\n","                    # Find the peak\n","                    peak = torch.max(actuals)\n","\n","                    # Move to numpy arrays\n","                    predictions = predictions.cpu().numpy()\n","                    actuals = actuals.cpu().numpy()\n","                    peak = peak.cpu().numpy()\n","\n","                    # Find the val_loss\n","                    val_loss, over, sla = evaluate_costs_single_clust(predictions, actuals, peak, alpha)\n","\n","\n","\n","                    # Determine the next state for the Actor Critic\n","                    complete_agent_post_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                    agent_post_state = np.ndarray(shape=(state_size,))\n","                    agent_post_state[0] = avg_validation_traffic\n","                    agent_post_state[1] = std_validation_traffic\n","                    for k in range(num_agents):\n","                        complete_agent_post_state[0,k] = agent_post_state\n","                    torch_post_state = [Variable(torch.Tensor(np.vstack(complete_agent_post_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","                    rewards = np.ndarray(shape=(rollout_threads, num_agents))\n","                    rewards[0,0] = -val_loss\n","\n","\n","                    # Save the experience in the replay buffer\n","                    replay_buffer.push(complete_agent_state, agent_actions, rewards, complete_agent_post_state)\n","\n","\n","\t\t            # Perform training of the Radio agent model\n","                    if len(replay_buffer) >= (ac_batch_size) and ac_iter < (ac_iterations - 5):\n","                        ac_model.prep_training(device='cpu')\n","                        sample = replay_buffer.sample(ac_batch_size, to_gpu=False, norm_rews=norm_rews)\n","                        ac_model.update_critic(sample)\n","                        ac_model.update_policies(sample)\n","                        ac_model.update_all_targets()\n","                        ac_model.prep_rollouts(device='cpu')\n","\n","                        # Perform a sample inference\n","                        complete_agent_state = np.ndarray(shape=(rollout_threads, num_agents), dtype=object)\n","                        agent_state = np.ndarray(shape=(state_size,))\n","                        agent_state[0] = avg_validation_traffic\n","                        agent_state[1] = std_validation_traffic\n","                        for k in range(num_agents):\n","                            complete_agent_state[0,k] = agent_state\n","                        torch_state = [Variable(torch.Tensor(np.vstack(complete_agent_state[:, j])), requires_grad=False) for j in range(num_agents)]\n","                        torch_action = ac_model.step(torch_state, explore=False)\n","                        agent_actions = [ac.data.numpy() for ac in torch_action]\n","                        test_tau = possible_tau[np.argmax(agent_actions[0])]\n","                        os.makedirs('Results/' + run_id, exist_ok=True)\n","                        np.save('Results/' + run_id + '/test_tau_ac_iter_' + str(ac_iter) + '.npy', tau)\n","\n","\n","                # Save the model\n","                file_path = os.path.join('AC_Models', run_id)\n","                model_path = os.path.join(file_path, 'ac_model_sim_' + str(num_run))\n","                os.makedirs(file_path, exist_ok=True)\n","                ac_model.save('AC_Models/' + run_id + '/ac_model_sim_' + str(num_run))\n","\n","\n","\n","                # Run the optimized model after taus optimization\n","                train_data, val_data, test_data, max_train, max_val, avg_val, std_val, max_test = get_data(epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, alpha, input_window, output_window, service, run)\n","\n","                # Model initialization\n","                run_id = service + '/Alpha_' + str(alpha) + '/Simulation_' + str(num_run)\n","                model = TransAm(max_train).to(device)\n","\n","                # Select loss function and optimizer and scheduler and learning rate\n","                criterion = AlphaLoss(alpha, 1, \"cuda\")\n","                lr = 0.005\n","                optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n","\n","                # Run model trainer\n","                for epoch in range(epochs):\n","                    train(model, optimizer, criterion, train_data, tau)\n","                    scheduler.step()\n","\n","                # Test the model\n","                loss, predictions, actuals, final_levels, final_max_levels = evaluate(model, criterion, test_data, epochs, tau)\n","\n","                # Find the peak\n","                peak = torch.max(actuals)\n","\n","                # Move to numpy arrays\n","                predictions = predictions.cpu().numpy()\n","                actuals = actuals.cpu().numpy()\n","                peak = peak.cpu().numpy()\n","\n","                # Find the different parts of alphaloss\n","                den_loss, over, sla = evaluate_costs_single_clust(predictions, actuals, peak, alpha)\n","                sla_cost = den_loss - over\n","                print(\"Denormalized Alpha-loss: \", den_loss)\n","\n","\n","                # Store the results\n","                os.makedirs('Results/' + run_id, exist_ok=True)\n","                np.save('Results/' + run_id + '/tes-transformer_predictions_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), predictions)\n","                np.save('Results/' + run_id + '/tes-transformer_actuals_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), actuals)\n","                np.save('Results/' + run_id + '/tes-transformer_alpha_loss_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), den_loss)\n","                np.save('Results/' + run_id + '/tes-transformer_over_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), over)\n","                np.save('Results/' + run_id + '/tes-transformer_sla_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), sla_cost)\n","                np.save('Results/' + run_id + '/tes-transformer_tau_%s_%d_%d_%d_%d.npy'%(service, num_clusters, run, num_run, alpha), tau)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["l8Q8pYyRqFQo"],"provenance":[]},"kernelspec":{"display_name":"ES-NN","language":"python","name":"es-nn"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}